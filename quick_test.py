#!/usr/bin/env python3
"""
ØªØ³Øª Ø³Ø±ÛŒØ¹ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
Quick Test Script for Real Data Evaluation
"""

import sys
from pathlib import Path
import torch
import numpy as np

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.envs.mock_env import MockSuctionEnv
from src.utils.gpu import get_device

def quick_test_model(model_path: str, num_episodes: int = 10):
    """ØªØ³Øª Ø³Ø±ÛŒØ¹ Ù…Ø¯Ù„"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ³Øª Ø³Ø±ÛŒØ¹ Ù…Ø¯Ù„...")
    
    # Setup
    device = get_device()
    print(f"ğŸ“± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡: {device}")
    
    # Load model
    try:
        checkpoint = torch.load(model_path, map_location=device)
        print("âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
        return
    
    # Create environment
    env = MockSuctionEnv(image_size=(128, 128), max_steps=1000)
    print("ğŸŒ Ù…Ø­ÛŒØ· Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
    
    # Test episodes
    success_count = 0
    total_rewards = []
    liquid_reductions = []
    contaminant_reductions = []
    
    print(f"\nğŸ¯ Ø´Ø±ÙˆØ¹ ØªØ³Øª {num_episodes} Ø§Ù¾ÛŒØ²ÙˆØ¯...")
    print("-" * 50)
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        total_reward = 0
        done = False
        step = 0
        
        while not done and step < 1000:
            # Simple random action for testing (replace with your model)
            action = env.action_space.sample()
            
            # Step environment
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            done = terminated or truncated
            step += 1
        
        # Calculate metrics
        liquid_reduction = info.get('liquid_reduction', 0) * 100
        contaminant_reduction = info.get('contaminant_reduction', 0) * 100
        success = liquid_reduction > 80 and contaminant_reduction > 80
        
        if success:
            success_count += 1
        
        total_rewards.append(total_reward)
        liquid_reductions.append(liquid_reduction)
        contaminant_reductions.append(contaminant_reduction)
        
        print(f"Ø§Ù¾ÛŒØ²ÙˆØ¯ {episode+1:2d}: Ù¾Ø§Ø¯Ø§Ø´={total_reward:6.2f}, Ù…Ø§ÛŒØ¹={liquid_reduction:5.1f}%, Ø¢Ù„ÙˆØ¯Ú¯ÛŒ={contaminant_reduction:5.1f}%, Ù…ÙˆÙÙ‚={'âœ…' if success else 'âŒ'}")
    
    # Calculate statistics
    success_rate = success_count / num_episodes
    mean_reward = np.mean(total_rewards)
    mean_liquid = np.mean(liquid_reductions)
    mean_contaminant = np.mean(contaminant_reductions)
    
    print("-" * 50)
    print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ ØªØ³Øª:")
    print(f"âœ… Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {success_rate:.1%}")
    print(f"ğŸ’° Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù¾Ø§Ø¯Ø§Ø´: {mean_reward:.2f}")
    print(f"ğŸ’§ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ø§Ù‡Ø´ Ù…Ø§ÛŒØ¹: {mean_liquid:.1f}%")
    print(f"ğŸ§ª Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ø§Ù‡Ø´ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ: {mean_contaminant:.1f}%")
    print(f"ğŸ¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§ÛŒ Ù…ÙˆÙÙ‚: {success_count}/{num_episodes}")
    
    # Performance assessment
    if success_rate >= 0.8:
        print("ğŸŒŸ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ!")
    elif success_rate >= 0.6:
        print("ğŸ‘ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨")
    elif success_rate >= 0.4:
        print("âš ï¸ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…ØªÙˆØ³Ø·")
    else:
        print("âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ØªØ³Øª Ø³Ø±ÛŒØ¹ Ù…Ø¯Ù„')
    parser.add_argument('--model', '-m', required=True, help='Ù…Ø³ÛŒØ± Ù…Ø¯Ù„')
    parser.add_argument('--episodes', '-e', type=int, default=10, help='ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§')
    
    args = parser.parse_args()
    
    if not Path(args.model).exists():
        print(f"âŒ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {args.model}")
        sys.exit(1)
    
    quick_test_model(args.model, args.episodes)
