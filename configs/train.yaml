# Training configuration
algorithm: ppo  # ppo or sac
device: cuda  # cuda or cpu

# PPO hyperparameters (optimized for GPU)
ppo:
  learning_rate: 1e-4  # lower for stability
  n_steps: 4096  # increased for better samples
  batch_size: 128  # increased for GPU
  n_epochs: 8  # reduced to prevent overfitting
  gamma: 0.995  # higher for long-term rewards
  gae_lambda: 0.95
  clip_range: 0.15  # slightly lower for stability
  ent_coef: 0.005  # lower entropy
  vf_coef: 0.3  # lower value function weight
  max_grad_norm: 0.5

# SAC hyperparameters (alternative)
sac:
  learning_rate: 3e-4
  buffer_size: 1000000
  learning_starts: 10000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: auto
  target_update_interval: 1
  target_entropy: auto

# Model architecture (optimized for GPU)
model:
  encoder_type: cnn  # cnn or vit
  hidden_dim: 512  # increased for better capacity
  num_layers: 4  # deeper network
  activation: relu
  dropout: 0.2  # higher dropout for regularization

# CNN encoder specific
cnn_encoder:
  channels: [32, 64, 128, 256]
  kernel_sizes: [3, 3, 3, 3]
  strides: [2, 2, 2, 2]
  padding: [1, 1, 1, 1]

# ViT encoder specific
vit_encoder:
  patch_size: 8
  embed_dim: 256
  num_heads: 8
  num_layers: 6
  mlp_ratio: 4.0

# Training settings (optimized for GPU)
total_timesteps: 2000000  # more training
eval_freq: 5000  # more frequent evaluation
save_freq: 25000  # more frequent saving
log_interval: 5  # more frequent logging
verbose: 1

# Checkpointing
checkpoint_freq: 10000
keep_checkpoints: 5
best_metric: safety_first_pareto

# Early stopping
early_stopping:
  enabled: false
  patience: 100000
  min_delta: 0.01
